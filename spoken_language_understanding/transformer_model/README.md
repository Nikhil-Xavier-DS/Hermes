# [Transformer based Spoken Language Understanding Model](https://github.com/Nikhil-Xavier-DS/Hermes/tree/master/spoken_language_understanding/transformer_model)
The code is implemented based on the publication, [Attention Is All You Need](https://arxiv.org/abs/1706.03762). 
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. The publication proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 

<img src="https://hub.packtpub.com/wp-content/uploads/2018/04/Attention.png" width="360">

#### Reference
1. Xiaodong Zhang and Houfeng Wang. 2016. "A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding". Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)
2. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017. "Attention Is All You Need". Proceeedings of EMNLP 2016
3. https://www.microsoft.com/en-us/research/project/spoken-language-understanding/
5. Git repository: https://github.com/zhedongzheng/tensorflow-nlp
6. https://www.tensorflow.org
7. https://github.com/yvchen/JointSLU/tree/master/data